---
title: Codewashing
sourceURL: 'https://adactio.com/journal/21886'
draft: false
pubDate: 2025-07-21
---

> I have little understanding for people using large language models to generate slop; words and images that nobody asked for.
>
> I have more understanding for people using large language models to generate code. Code isn’t the thing in the same way that words or images are; [code is the thing that gets you to the thing](https://youtu.be/QeY_5n75zPM?si=7Yajowz6aHEoL07u).

Leaving the ethics of LLM training aside, my biggest issue with AI-generated "slop" is that it's invariably shit.

I've read **so damn many** AI generated READMEs on GitHub in the last few weeks, and on at least two occasions lost my temper trying to find what I needed amongst the nonsense. It was better when lazy devs just had ten-line READMEs.

And I routinely have to wade through AI-generated slop articles in Google searches all the time.

In both cases, the end thing – **product** – the words. And those products have a shit user experience.

With code, the product is the working software. And the last few weeks have taught me that LLMs are **incredibly powerful** at helping me make decent user experiences quickly. It might matter to developers that the underlying code is slop, but if the working software is good it's the only _thing_ which matters to users.

> I genuinely look forward to being able to use a large language model with a clear conscience. Such a model would need to be trained ethically. When we get a free-range organic large language model I’ll be the first in line to use it. Until then, I’ll abstain.

I'm also looking forward to this. But until it arrives, I'm content to use un-ethical models for the same reason – if I'm trying to gain weight – I'll happily eat un-ethical meat in the absence of alternatives. **Doing so is has a sufficiently high impact on my quality of life that it outweighs my ethical concerns**.
